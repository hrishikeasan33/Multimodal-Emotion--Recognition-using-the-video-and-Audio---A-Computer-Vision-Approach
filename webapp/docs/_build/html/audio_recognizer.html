<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>audio_recognizer module &mdash; Multimodal Emotion Detection V1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="audio_recognizer8 module" href="audio_recognizer8.html" />
    <link rel="prev" title="app module" href="app.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Multimodal Emotion Detection
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">webapp</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="app.html">app module</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">audio_recognizer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="audio_recognizer8.html">audio_recognizer8 module</a></li>
<li class="toctree-l2"><a class="reference internal" href="video_recognizer.html">video_recognizer module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Training</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Multimodal Emotion Detection</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="modules.html">webapp</a> &raquo;</li>
      <li>audio_recognizer module</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/audio_recognizer.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="audio-recognizer-module">
<h1>audio_recognizer module<a class="headerlink" href="#audio-recognizer-module" title="Permalink to this heading"></a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="analyze_audio">
<span class="sig-name descname"><span class="pre">analyze_audio</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#analyze_audio" title="Permalink to this definition"></a></dt>
<dd><p>This function is called from the main app.py via the live-data route. It records a 3 seconds audio snippet, saves the snippet and passes it to helper functions to analyze it.
All important parameters can be set at the top: Sample rate, chunk size, seconds to be recorded, the microphone device index and the output filename. The audio recordings are done with the pyaudio library.
The function uses the pretrained audio recognition model and returns a predictions array of 4 class probabilities.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="get_audio_features">
<span class="sig-name descname"><span class="pre">get_audio_features</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#get_audio_features" title="Permalink to this definition"></a></dt>
<dd><p>This function was created for a better code structure. It loads the wave file using the librosa library. Then features are extracted using the <code class="docutils literal notranslate"><span class="pre">extract_audio_features()</span></code> function.
Returns a stacked array of features: without augmentation, with noise and streched / pitched. The model was trained with the same augmentation techniques.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="extract_audio_features">
<span class="sig-name descname"><span class="pre">extract_audio_features</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_rate</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#extract_audio_features" title="Permalink to this definition"></a></dt>
<dd><p>A helper function that extracts audio features from a given audio file using the librosa library. The features are the zero-crossing rate of the audio time series, a chromagram from the waveform, Mel-frequency cepstral coefficients (MFCCs), the root mean square value and finally a mel-scaled spectrogram.
The features are stacked horizontally in a numpy array.
This function also creates and saves diagrams that are shown in the frontend.</p>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="app.html" class="btn btn-neutral float-left" title="app module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="audio_recognizer8.html" class="btn btn-neutral float-right" title="audio_recognizer8 module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Ronja Rehm and Jan Kühlborn. Licensed under the MIT License.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>